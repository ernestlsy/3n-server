trainer:
  checkpoint_path: "./trainer/checkpoints/base"  # path to folder containing base model weights (.safetensors)
  lora_path: "./trainer/checkpoints/lora"  # path to folder containing LoRA config and weights
  output_path: "./temp"  # should be same as checkpoint_path for converter
  epoch_path: "./trainer/checkpoints/epochs" # path to folder where epoch checkpoints will be saved
  learning_rate: 2e-4
  weight_decay: 0.01
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  num_train_epochs: 3
  lora_dims: 64  # dimension of the smaller matrices
  lora_alpha: 128  # scaling factor
  lora_dropout: 0.1  # dropout of LoRA layers


converter:
  checkpoint_path: "./temp/", # directory where model.safetensors and tokenizer files are stored
  output_path: "./out",
  outtype: "q8_0" # quantization. choose from: "f32", "f16", "bf16", "q8_0", "tq1_0", "tq2_0", "auto"